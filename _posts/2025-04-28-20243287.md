---
layout: distill
title: Paper Review on Ewald-based Long-Range Message Passing for Molecular Graphs and Protein Language Model Fitness is a Matter of Preference
description: This post reviews two recent papers on how to overcome long ranged interaction in message passing for large molecular graphs using a well-known concept from molecular dynamics and how training data distribution affect overall model performance in the realm of protein language models.
tags: distill formatting
date: 2025-04-28
giscus_comments: true
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

# must be the exact same name as your blogpost
# bibliography: 2025-04-28-20243287.bib

toc:
  - name: What do output unit activities really mean?
  - name: Previous studies on this topic
  - name: Deriving the classification objective using maximum likelihood
  - name: Interpretation of outputs as the Bayesian posterior
    subsections:
      - name: Minimizing divergence between outputs and the Bayesian posterior
      - name: Loss is minimized when outputs exactly match the posterior
      - name: How to interpret the network outputs
  - name: Empirical studies with known generative models
    subsections:
      - name: A simple classification example
      - name: A harder example with a more complex posterior
      - name: An even harder classification example
  - name: Conclusions and discussions
---

## What do output unit activities really mean?

Deep neural networks trained for classification tasks have been a key driver in the rise of modern deep learning. Training networks for classification is often one of the first lessons or tutorials people encounter when they begin their journey with deep learning. In classification tasks, the [multi-class classification](https://en.wikipedia.org/wiki/Multiclass_classification) problem is one of the most common types of classification problems that people encounter. Widely used datasets, like MNIST, CIFAR-10/100, and ImageNet, are all geared toward the multi-class classification problem. In this problem, the model is trained to receive some data $$X$$ as input and infer its class label $$C$$ from one of the $$M$$ total possible classes. Each data point has one and only one class label from $$M$$ possible classes $$c \in \{1 ... M\}$$. For example, if we train a convolutional neural network (CNN) to do classification on ImageNet, the data $$X$$ is the input image, and the output class label $$C$$ is one of the 1000 classes in the ImageNet dataset.

## Conclusions and discussions

In this tutorial, we explored how to interpret the output activations of neural networks trained on classification tasks. Through theoretical derivations, we demonstrated that these activations, after applying the softmax function, should closely approximate the Bayesian posterior probabilities. Specifically, minimizing the cross-entropy loss function commonly used in classification is equivalent to minimizing the conditional KL divergence between the network's outputs and the Bayesian posterior, and the loss is minimized when the network outputs exactly match the Bayesian posterior.

Our empirical studies illustrated that neural networks indeed approximate the posterior well in simpler classification scenarios. However, as the complexity of the posterior increases or as data becomes sparse in certain regions, accurate approximation becomes significantly more challenging. These findings emphasize that while neural network outputs can be interpreted as the approximation to the Bayesian posterior, the quality of the approximation depends heavily on factors such as posterior complexity and data availability. The approximation quality will also depends on the network architecture and optimization details, which we did not explore here. 

It is worth noting that the theoretical derivation we presented here only applies to classification tasks where the output is a discrete random variable that takes a finite set of values. The interpretation of network outputs in other cases, such as regression tasks, will depends on the specific formulations.

We hope that clarifying the interpretation of classification network outputs encourages us to view them not just as simple predictions, but as rich probabilistic representations that reflect uncertainty and ambiguity inherent in the data.
