---
layout: distill
title: AI810 Blog Post (20243287)
description: "This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025)."
tags: distill formatting
date: 2025-04-28
giscus_comments: true
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

# must be the exact same name as your blogpost
# bibliography: 2025-04-28-20243287.bib

toc:
  - name: Motivation for this review
  - name: Ewald-based Long-Range Message Passing for Molecular Graphs
    subsections:
      - name: Ewald-summation from molecular dynamics
      - name: Ewald message passing
      - name: Results on long-ranged interaction
  - name: Protein Language Model Fitness is a Matter of Preference
    subsections:
      - name: Recent works reveal limitations of SoTA PLMs
      - name: What is preference? Why preference?
      - name: Single-inference pseudo log likelihood
      - name: Influence diminishes with edit distance
      - name: A novel fine-tuning pipeline based on PLL
  - name: Conclusions and discussions
---


## Motivation for this review
Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology.

Early encodings---one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors---laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function.

In response, protein language models (PLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids---and extending them to atom-level resolution---continues to pose a significant challenge.

Meanwhile, graph neural networks convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules.

Similarly, how to effectively leverage multi-scale PLMs into a particular protein domain remains elusive.

This review examines two recent studies designed to close these gaps.

First, we revisit Ewald Summation---long established in molecular dynamics for treating infinite periodic systems---and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs.

Then, we explore a fine-tuning paradigm for PLMs that how we can strategically adapt large, pretrained PLMs into specific protein domain without compromising their ability.

Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability.

Whether it's importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained PLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential---and challenging those assumptions often leads to the deepest insights.

## Ewald-based Long-Range Message Passing for Molecular Graphs


## Protein Language Model Fitness is a Matter of Preference


## Conclusions and discussions

