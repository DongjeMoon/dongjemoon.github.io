---
layout: distill
title: AI810 Blog Post (20243287)
description: "This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025)."
tags: distill formatting
date: 2025-04-28
giscus_comments: true
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true
future: true
htmlwidgets: true

# must be the exact same name as your blogpost
bibliography: 2025-04-28-20243287.bib

toc:
  - name: Motivation for this review
  - name: Ewald-based Long-Range Message Passing for Molecular Graphs
    subsections:
      - name: Why does long-range matters in GNN?
      - name: What is Ewald Summation and where is it useful?
      - name: Ewald Message Passing
      - name: Results on long-ranged interaction
  - name: Protein Language Model Fitness is a Matter of Preference
    subsections:
      - name: Recent works reveal limitations of SoTA PLMs
      - name: What is preference? Why preference?
      - name: Single-inference pseudo log likelihood
      - name: Influence diminishes with edit distance
      - name: A novel fine-tuning pipeline based on PLL
  - name: Conclusions and discussions
---


## Motivation for this review
Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology. Early encodings---one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors---laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function. In response, protein language models (PLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids---and extending them to atom-level resolution---continues to pose a significant challenge. Meanwhile, graph neural networks (GNN) convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules. Similarly, how to effectively leverage multi-scale PLMs into a particular protein domain remains elusive.

With this blog post, we examine two recent studies designed to close these gaps. First, we revisit Ewald Summation---long established in molecular dynamics for treating infinite periodic systems---and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs. Then, we explore a fine-tuning paradigm for PLMs that how we can strategically adapt large, pretrained PLMs into specific protein domain without compromising their performance. Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability. Whether it's importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained PLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential---and challenging those assumptions often leads to the deepest insights.

## Ewald-based Long-Range Message Passing for Molecular Graphs
### Why does long-range matters in GNN?
In standard Message Passing Neural Networks (MPNNs) for molecular systems, each atomâ€™s representation is updated by aggregating information from the neighbors within a **fixed distance cutoff**. While this effectively captures local chemical interactions especially at the Dalton scale, it systematically neglects long-range forces such as electrostatics, dispersion, and allosteric effects in larger molecules (often kilodalton to megadalton scale).

This locality bias in large graphs gives to over-squashing problem of GNNs <d-cite key="alon2021bottleneck"></d-cite>. Information from distant nodes must traverse many intermediate aggregations, compressing rich signals into narrow bottlenecks and degrading the network's ability to learn non-local dependencies. Therefore, multiple strategies, including sensitivity analysis and graph-rewiring, have been addressed to solve this problem in long-range interaction between a source and target vertex.

In particular, we will explore how the **Ewald Summation** principle can be reformulated as an agnostic message-passing module to directly mitigate over-squashing and endow geometric GNNs with a true global context.

### What is Ewald Summation and where is it useful?
Before we define Ewald Summation, we must first introduce the notion of **Periodic Boundary Conditions** (PBC) and **Supercell lattice**. Under PBCs, a finite simulation box is conceptually tiled infinitely in every direction (see Figure 1). This wrapping ensures every atom experience a uniform environment and eliminates any edge artifacts. The Supercell lattice specifies the integer combinations of the box's basis vectors that generate all image cells (the replicas)---atom embeddings in the primary cell thus can be replicated implicitly across these translations when computing interactions.

Without PBCs, a na&iuml;ve sum of Coulomb potential over an infinite lattice is only conditionally convergent and thus computationally infeasible. The solution lies in recognizing that the direct lattice has a corresponding reciprocal lattice in Fourier space. Ewald Summation exploits this dual structure by decomposing the problematic infinite sum ($\Phi(r)$) into two rapidly, absolutely convervent series: short-range interaction ($\Phi^{sr}(r)$) and long-range interaction ($\Phi^{lr}(r)$).

\begin{equation}
\begin{split}
    \Phi(r) = \Phi_{\mathrm{sr}}(r) + \Phi_{\mathrm{lr}}(r), \\ 
    r = \lVert \mathbf{x}_i - \mathbf{x}_j\rVert.
\end{split}
\end{equation}

### Ewald Message Passing
Authors of this paper adopt this exact decomposition of $\Phi(r)$ as the foundation of the Ewald Message Passing (EMP) module, which can serve as an augmentation on top of existing MPNN architectures. The short-range convolution, $\Phi^{sr}(r)$, makes use of the continuous-filter convolution employed by any geometric GNN that implements distance-dependant message function (e.g. SchNet, PaiNN, DimeNet++, GemNet-T). Here, an atom gathers information from all neighbors within a prescribed Euclidean cutoff by applying a learnable radial filter to inter-atomic distances. This preserves the model's capacity to capture local chemical phenomena without any explicit modification into the pre-existing GNN architecture.

To implement the long-range stream in a graph context, they first define a **structure factor**, $s_{k}$, a notion from crystallography, as a Fourier space representation of atom embeddings, which defines a meaure of how node embeddings, $h_{j}$, are collectively scatter at frequency *k* (i.e. a weighted sum over *all* supercell embeddings). To obtain $s_{k}$, $h_{j}$'s are projected to the reciprocal space by computing over a fixed, learnable set of wavevectors {*k*}.

\begin{equation}
\begin{split}
    s_{\mathbf{k}} 
    &= \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}^{T}\mathbf{x}_{j}\bigr)\,.
\end{split}
\end{equation}

Once in the reciprocal space, each mode is modulated by a learnable frequency filter, $\hat{\Phi}_{\mathrm{lr}}(\|\mathbf{k}\|)$, yielding

\begin{equation}
\begin{split}
    \hat{M}^{\mathrm{lr}}(\mathbf{x_i})
    &= s_{\mathbf{k}}\;\hat{\Phi}_{\mathrm{lr}}\bigl(\|\|\mathbf{k}\|\|\bigr)\,.
\end{split}
\end{equation}

Next, $s_{k}$ is projected back into the real position space through inverse transformation

\begin{equation}
\begin{split}
    M_i^{\mathrm{lr}}
    &= \sum_{\mathbf{k}} \hat{M}^{\mathrm{lr}}(\mathbf{k})\;exp({\,i\,\mathbf{k}\cdot \mathbf{x}_i})\,,
\end{split}
\end{equation}

Finally, $M^{lr}$ is added to $M^{sr}$, forming a complete update cycle for a single layer of GNN-EMP architecture

\begin{equation}
\begin{split}
    h_i^{(\ell+1)}
    &= \frac{1}{\sqrt{3}}\Bigl(h_i^{(\ell)}
    + f_{\mathrm{upd}}^{\mathrm{sr}}\bigl(M_i^{\mathrm{sr}}\bigr)
    + f_{\mathrm{upd}}^{\mathrm{lr}}\bigl(M_i^{\mathrm{lr}}\bigr)\Bigr)\,.
\end{split}
\end{equation}



## Protein Language Model Fitness is a Matter of Preference


## Conclusions and discussions

