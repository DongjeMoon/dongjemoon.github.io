@inproceedings{alon2021bottleneck,
  title={On the Bottleneck of Graph Neural Networks and its Practical Implications},
  author={Alon, Yahav},
  journal={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/pdf?id=i80OPhOCVH2},
  publisher={OpenReview.net}
}

@inproceedings{NIPS2017_303ed4c6,
  author = {Sch\"{u}tt, Kristof and Kindermans, Pieter-Jan and Sauceda Felix, Huziel Enoc and Chmiela, Stefan and Tkatchenko, Alexandre and M\"{u}ller, Klaus-Robert},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {SchNet: A continuous-filter convolutional neural network for modeling quantum interactions},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
  volume = {30},
  year = {2017}
}

@misc{gasteiger2022fastuncertaintyawaredirectionalmessage,
      title={Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules}, 
      author={Johannes Gasteiger and Shankari Giri and Johannes T. Margraf and Stephan Günnemann},
      year={2022},
      eprint={2011.14115},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2011.14115}, 
}

@misc{schütt2021equivariantmessagepassingprediction,
      title={Equivariant message passing for the prediction of tensorial properties and molecular spectra}, 
      author={Kristof T. Schütt and Oliver T. Unke and Michael Gastegger},
      year={2021},
      eprint={2102.03150},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.03150}, 
}

@misc{gasteiger2024gemnetuniversaldirectionalgraph,
      title={GemNet: Universal Directional Graph Neural Networks for Molecules}, 
      author={Johannes Gasteiger and Florian Becker and Stephan Günnemann},
      year={2024},
      eprint={2106.08903},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/2106.08903}, 
}


@inproceedings{weinstein2022nonidentifiability,
  title={Non-identifiability and the Blessings of Misspecification in Models of Molecular Fitness},
  author={Eli N Weinstein and Alan Nawzad Amin and Jonathan Frazer and Debora Susan Marks},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=CwG-o0ind6t}
}

@article{Ding2024.03.07.584001,
	author={Ding, Frances and Steinhardt, Jacob},
	title={Protein language models are biased by unequal sequence sampling across the tree of life},
	elocation-id={2024.03.07.584001},
	year={2024},
	doi={10.1101/2024.03.07.584001},
	publisher={Cold Spring Harbor Laboratory},
	abstract={Protein language models (pLMs) trained on large protein sequence databases have been used to understand disease and design novel proteins. In design tasks, the likelihood of a protein sequence under a pLM is often used as a proxy for protein fitness, so it is critical to understand what signals likelihoods capture. In this work we find that pLM likelihoods unintentionally encode a species bias: likelihoods of protein sequences from certain species are systematically higher, independent of the protein in question. We quantify this bias and show that it arises in large part because of unequal species representation in popular protein sequence databases. We further show that the bias can be detrimental for some protein design applications, such as enhancing thermostability. These results highlight the importance of understanding and curating pLM training data to mitigate biases and improve protein design capabilities in under-explored parts of sequence space.Competing Interest StatementThe authors have declared no competing interest.},
	URL={https://www.biorxiv.org/content/early/2024/03/12/2024.03.07.584001},
	eprint={https://www.biorxiv.org/content/early/2024/03/12/2024.03.07.584001.full.pdf},
	journal={bioRxiv}
}

@InProceedings{pmlr-v261-hermann24a,
  title={Beware of Data Leakage from Protein LLM Pretraining},
  author={Hermann, Leon and Fiedler, Tobias and Nguyen, Hoang An and Nowicka, Melania and Bartoszewicz, Jakub M},
  booktitle={Proceedings of the 19th Machine Learning in Computational Biology meeting},
  pages={106--116},
  year={2024},
  editor={Knowles, David A and Mostafavi, Sara},
  volume={261},
  series={Proceedings of Machine Learning Research},
  month={05--06 Sep},
  publisher={PMLR},
  pdf={https://raw.githubusercontent.com/mlresearch/v261/main/assets/hermann24a/hermann24a.pdf},
  url={https://proceedings.mlr.press/v261/hermann24a.html},
  abstract={Pretrained protein language models are becoming increasingly popular as a backbone for protein property inference tasks such as structure prediction or function annotation, accelerating biological research. However, related research oftentimes does not consider the effects of data leakage from pretraining on the actual downstream task, resulting in potentially unrealistic performance estimates. Reported generalization might not necessarily be reproducible for proteins highly dissimilar from the pretraining set.  In this work, we measure the effects of data leakage from protein language model pretraining in the domain of protein thermostability prediction. Specifically, we compare two different dataset split strategies: a pretraining-aware split, designed to avoid similarity between pretraining data and the held-out test sets, and a commonly-used naive split, relying on clustering the training data for a downstream task without taking the pretraining data into account. Our experiments suggest that data leakage from language model pretraining shows consistent effects on melting point prediction across all experiments, distorting the measured performance by an average 11.1% compared to the pretraining-aware split. The source code and our dataset splits are available at https://gitlab.com/dacs-hpi/pretraining-aware-hotprot.}
}

@misc{wang2019bertmouthspeakbert,
      title={BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model}, 
      author={Alex Wang and Kyunghyun Cho},
      year={2019},
      eprint={1902.04094},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1902.04094}, 
}

@misc{isaacs_pbc,
  author={{Isaacs.sourceforge.io}},
  title={Periodic Boundary Conditions},
  howpublished={\url{https://isaacs.sourceforge.io/phys/pbc.html}},
}

@misc{rampášek2023recipegeneralpowerfulscalable,
      title={Recipe for a General, Powerful, Scalable Graph Transformer}, 
      author={Ladislav Rampášek and Mikhail Galkin and Vijay Prakash Dwivedi and Anh Tuan Luu and Guy Wolf and Dominique Beaini},
      year={2023},
      eprint={2205.12454},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.12454}, 
}

@misc{li2024neuralatomspropagatinglongrange,
      title={Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel}, 
      author={Xuan Li and Zhanke Zhou and Jiangchao Yao and Yu Rong and Lu Zhang and Bo Han},
      year={2024},
      eprint={2311.01276},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.01276}, 
}

@misc{adamczyk2025molecularfingerprintsstrongmodels,
      title={Molecular Fingerprints Are Strong Models for Peptide Function Prediction}, 
      author={Jakub Adamczyk and Piotr Ludynia and Wojciech Czech},
      year={2025},
      eprint={2501.17901},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM},
      url={https://arxiv.org/abs/2501.17901}, 
}
