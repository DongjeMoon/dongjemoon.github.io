<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="motivation-for-this-review">Motivation for this review</h2> <p>Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology. Early encodings—one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors—laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function. In response, protein language models (pLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids—and extending them to atom-level resolution—continues to pose a significant challenge. Meanwhile, graph neural networks (GNN) convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules. Similarly, how to effectively leverage multi-scale pLMs into a particular protein domain remains elusive.</p> <p>With this blog post, we examine two recent studies designed to close these gaps. First, we revisit Ewald Summation—long established in molecular dynamics for treating infinite periodic systems—and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs. Then, we explore a fine-tuning paradigm for pLMs that how we can strategically adapt large, pretrained pLMs into specific protein domain without compromising their performance. Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability. Whether it’s importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained pLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential—and challenging those assumptions often leads to the deepest insights.</p> <h2 id="ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</h2> <h3 id="why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</h3> <p>In standard Message Passing Neural Networks (MPNNs) for molecular systems, each atom’s representation is updated by aggregating information from the neighbors within a <strong>fixed distance cutoff</strong>. While this effectively captures local chemical interactions especially at the Dalton scale, it systematically neglects long-range forces such as electrostatics, dispersion, and allosteric effects in larger molecules (often kilodalton to megadalton scale).</p> <p>This locality bias in large graphs gives rise to over-squashing problem of GNNs <d-cite key="alon2021bottleneck"></d-cite>. Information from distant nodes must traverse many intermediate aggregations, compressing rich signals into narrow bottlenecks and degrading the network’s ability to learn non-local dependencies. Therefore, multiple strategies, including sensitivity analysis and graph-rewiring, have been addressed to solve this problem in long-range interaction between a source and target vertex.</p> <p>In particular, we will explore how the <strong>Ewald Summation</strong> principle can be reformulated as an agnostic message-passing module to directly mitigate over-squashing and endow geometric GNNs with a true global context.</p> <h3 id="what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</h3> <p>Before we define Ewald Summation, we must first introduce the notion of <strong>Periodic Boundary Conditions</strong> (PBC) and <strong>Supercell lattice</strong>. Under PBCs, a finite simulation box is conceptually tiled infinitely in every direction (see Figure 1). This wrapping ensures every atom experience a uniform environment and eliminates any edge artifacts. The Supercell lattice specifies the integer combinations of the box’s basis vectors that generate all image cells (the replicas)—atom embeddings in the primary cell thus can be replicated implicitly across these translations when computing interactions.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <img class="invert" style="max-width: 50%; height: auto;" src="/assets/img/2025-04-28-20243287/pbc.webp" frameborder="0" scrolling="no"> <figcaption class="caption"> Schematic representation of periodic boundary conditions <d-cite key="isaacs_pbc"></d-cite> </figcaption> </figure> <p>Without PBCs, a naïve sum of Coulomb potential of ions with distance cutoff over an infinite lattice is only conditionally convergent and thus computationally infeasible. (<em>e.g.</em> the long tail problem from the inversely proportional to the distance). The solution lies in recognizing that the direct lattice can be approximated using reciprocal lattice in Fourier space with a frequency cutoff. Ewald Summation exploits this dual structure by decomposing the problematic infinite sum ($\Phi(r)$) into two rapidly, absolutely convergent series: short-range interaction ($\Phi^{sr}(r)$) and long-range interaction ($\Phi^{lr}(r)$), which can be expressed in the Fourier space as $\hat\Phi^{lr}(\left| \cdot \right|)$.</p> <p>\begin{equation} \begin{split} \Phi(r) = \Phi^{\mathrm{sr}}(r) + \Phi^{\mathrm{lr}}(r), \ <br> r = \lVert \mathbf{x}_i - \mathbf{x}_j\rVert. \end{split} \end{equation}</p> <h3 id="ewald-message-passing">Ewald Message Passing</h3> <p>Authors of this paper adopt this exact decomposition of $\Phi(r)$ as the foundation of the Ewald Message Passing (EMP) module, which can serve as an augmentation on top of existing MPNN architectures. The short-range convolution, $\Phi^{sr}(r)$, makes use of the continuous-filter convolution employed by any geometric GNN that implements distance-dependent message function (<em>e.g.</em> SchNet <d-cite key="NIPS2017_303ed4c6"></d-cite>, PaiNN <d-cite key=" schütt2021equivariantmessagepassingprediction"></d-cite>, DimeNet++ <d-cite key=" gasteiger2022fastuncertaintyawaredirectionalmessage"></d-cite>, GemNet-T <d-cite key=" gasteiger2024gemnetuniversaldirectionalgraph"></d-cite>). Here, an atom gathers information from all neighbors within a prescribed Euclidean cutoff by applying a learnable radial filter to inter-atomic distances. This preserves the model’s capacity to capture local chemical phenomena without any explicit modification into the pre-existing GNN architecture.</p> <p>To implement the long-range stream in a graph context, they first define a <strong>structure factor</strong>, $s_{k}$, a notion from crystallography, as a Fourier space representation of atom embeddings, which defines a measure of how node embeddings, $h_{j}$, are collectively scatter at frequency <em>k</em> (<em>i.e.</em> a weighted sum over <strong>all</strong> supercell embeddings).</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/ewald_sum.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Three main process of Ewald Message Passing: first, group of atom embeddings are projected to Fourier space as structure factor embeddings. Next, the embeddings are multiplied k-cutoff-wise with learned filter values. Lastly, the reciprocal embeddings are projected back into real space. </figcaption> </figure> <p>To obtain $s_{k}$, $h_{j}$’s are projected to the reciprocal space by computing over a fixed, learnable set of wavevectors {<em>k</em>}</p> <p>\begin{equation} \begin{split} s_{\mathbf{k}} &amp;= \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}^{T}\mathbf{x}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Once in the reciprocal space, $\Lambda^{\prime}$, each mode is modulated by a learnable frequency filter, $\hat{\Phi}_{\mathrm{lr}}(||\mathbf{k}||)$, yielding</p> <p>\begin{equation} \begin{split} \hat{M}^{\mathrm{lr}}(\mathbf{x_i}) &amp;= s_{\mathbf{k}}\cdot\hat{\Phi}^{\mathrm{lr}}\bigl(||\mathbf{k}||\bigr)\,. \end{split} \end{equation}</p> <p>Next, $s_{k}$ is projected back into the real position space through inverse transformation</p> <p>\begin{equation} \begin{split} M_i^{\mathrm{lr}} &amp;= \sum_{\mathbf{k}\in\Lambda^{\prime}} \hat{M_{k}^{\mathrm{lr}}}\;exp({\,i\,\mathbf{k}\cdot \mathbf{x}_i})\,, \end{split} \end{equation}</p> <p>Finally, $M^{lr}$ is added to $M^{sr}$, forming a complete update cycle for a single layer of GNN-EMP architecture</p> <p>\begin{equation} \begin{split} h_i^{(\ell+1)} &amp;= \frac{1}{\sqrt{3}}\Bigl(h_i^{(\ell)} + f_{\mathrm{upd}}^{\mathrm{sr}}\bigl(M_i^{\mathrm{sr}}\bigr) + f_{\mathrm{upd}}^{\mathrm{lr}}\bigl(M_i^{\mathrm{lr}}\bigr)\Bigr)\,. \end{split} \end{equation} Authors employ two mode-generation schemes depending on whether the system is periodic or aperiodic. For an aperiodic case, there is no reciprocal lattice $\Lambda^{\prime}$, so the structure factor, $s_k$ map is first regarded as a continuous function from $\mathbb{R}^3$ to the D-dimensional complex numbers.</p> <p>\begin{equation} \begin{split} s(k): \mathbb{R}^3 &amp;\to \mathbb{C}^d,<br> \mathbf{k} &amp;\mapsto \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}\cdot\mathbf{r}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Because the real-space supercell size and the spacing of admissible k-vectors are inversely related, this field must be sampled on a finite grid. Therefore, they place a uniform voxel grid of <em>k</em> wavevectors and replace all continuous functions by its voxel averages. To prevent aliasing and to guarantee rotational invariance, the grid is defined in a co-rotating frame obtained from principal component analysis (PCA) of the atomic point cloud: the three PCA eigen-vectors (ordered by descending eigen-value) form an orthonormal basis that rotates with the molecule. Rotating the molecule in real space then rotates the voxel grid in the reciprocal space, keeping the sampled modes and their learned filter unaffected by the global orientation.</p> <p>Similarly, <strong>frequency filters</strong>, $\hat\Phi(||\cdot||)$, are parameterized differently in the two settings. In aperiodic cases, they employ the same radial-basis parametrization used in ScheNet: $\hat\Phi(||k||)$ is expressed as a linear combination of Gaussian radial basis functions whose centers lie within k-cutoff radius, $c_k$ in Fourier space. Because the filter depends on the magnitude of $||k||$, it is rotation-invariant once the molecule has been aligned to its PCA frame. However, for periodic systems, a radial envelope would be overly restrictive; the periodic boundary conditions carry valuable geometric information. Instead, the filter is non-radial: a separate complex coefficient is learned for every point-symmetry orbit in the reciprocal lattice $\Lambda^{\prime}$. Because the reciprocal lattice co-rotates with the supercell lattice, the learned weights remain rotation-invariant, even though the filter is no longer radial.</p> <h3 id="results-on-long-range-interaction">Results on long-range interaction</h3> <p>The authors evaluate EMP module on two public data sets with contrasting geometry:</p> <ul> <li>OC20 — 133M surface-adsorbate structures from density functional theory (DFT) relaxations, ~50-200 atoms each</li> <li>OE62 — 62k organic molecules, energies include D3 dispersion, ~50-100 atoms each</li> </ul> <p>As baselines, four geometric GNN models are considered: the node-based models SchNet and PaiNN, and the edge-based models DimeNet++ and GemNet-T. Each EMP variant is parameter-matched to its baseline by reducing the hidden size so that both networks contain the same total number of trainable parameters. Energy mean absolute error (EMAE) is used as the evaluation metric.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/ewald_result1.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Cost vs. EMAE on OE62 and OC20, respectively. </figcaption> </figure> <p>For both datasets, adding EMP consistently lowers EMAE on all four baseline models. However, for OE62, the edge-based models gain trivial improvement because a 12-angstrom radius already saturates nearly every graph in the dataset; therefore, long-range information processed by EMP is partly redundant.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <div style="display: flex; justify-content: center; gap: 1rem;"> <img src="/assets/img/2025-04-28-20243287/ewald_result2.png" style="width: 50%; height: auto;"> <img src="/assets/img/2025-04-28-20243287/ewald_result3.png" style="width: 50%; height: auto;"> </div> <figcaption class="caption" style="width: 100%; text-align: center;"> Ewald-MP vs. DFT-D3 correction for EMAE on OE62. </figcaption> </figure> <p>To test whether the improvement stems from capturing long-rage interaction rather than from an architectural artefact, the authors compare EMP to an explicit DFT-D3 correction on OE62. Figure 4 shows a strong positive correlation between D3 energy magnitude and the EMAE gain delivered by EMP. Moreover, for SchNet and DimeNet++ the learned correction outperforms the hand-crafted D3 term when both are applied to the same baseline. This indicates that EMP is not only learning Van der Waals force without any hand-crafted correction but also enriching embeddings with auxiliary factors as well.</p> <h3 id="limitation">Limitation</h3> <p>While EMP introduces a compelling paradigm to model long-range interactions in geometric GNNs, it also comes with several limitations. One notable limitation of the current formulation is that the model performance succeeds only on energy prediction, while only subtle improvement observed in force MAE. This is important because molecular dynamics or structure optimization depends heavily on forces. However, since forces are <strong>energy gradients</strong> more dominated by fast-changing short wavelength terms and EMP focuses on the long wavelength terms within the prescribed frequency cutoff, \begin{equation} \begin{split} \vec{F}(\vec{x_i}) = -\vec{\nabla}V(\vec{x_i}) = -\sum_{\vec{k}}i\vec{k}\hat{V_{\vec{k}}}exp(i\vec{k}^T\vec{x}) \end{split} \end{equation} EMP becomes more of an energy correction scheme rather than force correction scheme.</p> <p>Another limitation of the current EMP framework lies in the reliance on <strong>fixed input geometries</strong>. The long-range interactions, computed via structure factor embeddings in the reciprocal space, are determined only once initially based on the input atomic positions and do not adapt throughout the message passing process. This static formulation contrasts with equivariant architectures like PaiNN or GemNet-T, where both scalar and vector features evolve jointly over layers in response to updated representations. As such, EMP does not adjust the k-space contributions in response to intermediate feature updates, which may limit the model’s capacity to learn feedback effects between short- and long-range interactions during inference.</p> <p>Similarly, the implementation of EMP in the paper operates exclusively on atom type and atomic coordinate embeddings. It does not incorporate vectorial features (<em>e.g.,</em> dipoles, angular momenta) or multi-modal information (<em>e.g.,</em> electronic density, spectroscopy). Extending EMP to such modalities would require significant architectural changes, particularly to preserve equivariance and rotational consistency in the Fourier domain.</p> <h2 id="protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</h2> <h3 id="recent-works-reveal-limitations-of-state-of-the-art-protein-language-models">Recent works reveal limitations of state-of-the-art protein language models</h3> <p>pLMs, such as ESM, MSA Transformer, EVE, ProGen, and TranceptEVE, have shown great capacity in inferring evolutionary trajectories, guiding various downstream tasks including protein design and predicting zero-shot mutational effects. Despite their success, recent studies highlight critical limitations, revealing that the pLMs are significantly influenced by biases present in their training data—biases that do not necessarily reflect true evolutionary fitness.</p> <ul> <li>pLMs primarily model the underlying distribution of the training data rather than genuine biological fitness functions <d-cite key="weinstein2022nonidentifiability"></d-cite>.</li> <li>Training data are often biased towards frequently observed sequences rather than evolutionarily advantageous ones <d-cite key="Ding2024.03.07.584001"></d-cite>.</li> <li>Benchmarks used for model evaluation sometimes overlap with sequences used for training pLM, inflating the resultant model performance <d-cite key="pmlr-v261-hermann24a"></d-cite>.</li> </ul> <p>In other words, a sequence’s evolutionary frequency does not inherently correlate with biological superiority. To address this discrepancy, the authors introduce the concept of <strong>preference</strong> and <strong>influence</strong> to systematically investigate pLM performance.</p> <h3 id="what-is-preference-why-preference">What is preference? Why preference?</h3> <p>The notion of preference stems from Bradly-Terry model of preference, which quantifies the probability of selecting one item over another: \begin{equation} \begin{split} Pr(i &gt; j) = \frac{p_i}{p_i + p_j} \end{split} \end{equation} Where $p_i$ and $p_j$ denote Protein $i$ and Protein $j$, respectively. Preference models are well-established in various fields, notably direct preference optimization (DPO) and Elo rating systems. Within the context of pLMs, the preference translates into the probability that Protein $i$ is preferred over Protein $j$ is proportional to their sequence likelihood score. Hence, pLM implicitly prioritize sequences they recognize as evolutionarily familiar or plausible.</p> <p><strong>What is causing this sequence preference—model architecture or training data?</strong> Intuition suggests that training data composition strongly influences these preferences, as pLMs likely reflect biases inherent in their datasets rather acquiring genuine biological insights. The authors similarly hypothesize that observed sequence preferences originate primarily from the training data rather than intrinsic model properties.</p> <p>To explore this hypothesis, the authors focus their study on protein fitness prediction, specifically leveraging deep mutational scan (DMS) datasets. These datasets assess the relative fitness of numerous protein mutants, assigning wild type (WT) fitness as a baseline (fitness score = 1), with mutants scoring greater or lesser values depending on their relative performance, where greater values mean better fitness.</p> <p>The widely-used zero-shot fitness scoring function compares mutants $x’$ against WT $x$ sequences as follows:</p> \[\begin{split} f(x^{\prime},x) &amp;= \sum_{t \in T} \Bigl( \log P\bigl(y_{t} = x^{\prime}_{t} \mid x_{\setminus t},\,\theta\bigr) \\[-0.3em] &amp;\quad\quad -\,\log P\bigl(y_{t} = x_{t} \mid x_{\setminus t},\,\theta\bigr) \Bigr). \end{split}\] <p>While commonly employed and benchmarked against experimental fitness via Spearman correlations, this formula introduces two significant limitations:</p> <ul> <li> <strong>Linear Additivity Assumption</strong>: It assumes independence of individual mutations by summing their log-likelihood differences. However, real-world proteins frequently exhibit epistasis, where mutation effects are interdependent and non-linear. Consequently, the formula fails to capture complex interactions between multiple mutations.</li> <li> <strong>Sequence Length Constraint</strong>: It requires $x$ and $x’$ to have the same sequence length and forces residue-to-residue alignment between WT and mutant sequences. Therefore, it excludes insertion and deletions (indel) mutations, which are naturally prevalent.</li> </ul> <p>Recognizing these constraints, the authors seek to understand the influence of training data on pLM-derived preferences while working within these formula-imposed boundaries.</p> <h3 id="single-inference-pseudo-log-likelihood">Single-inference pseudo log-likelihood</h3> <p>Unlike autoregressive language models, masked language models cannot compute joint likelihood naturally. To address this limitation, Wang &amp; Cho introduced pseudo log-likelihood (PLL) <d-cite key="wang2019bertmouthspeakbert"></d-cite>:</p> <p>\begin{equation} \begin{split} \mathrm{PLL}(x) \;=\; \frac{1}{L} \sum_{i=1}^{L} \log P\bigl(y_{i} = x_{i} \,\bigm|\, x_{\setminus i},\,\theta\bigr) \end{split} \end{equation}</p> <p>However, this PLL formulation suffers from $O(L)$ forward passes—one per masked residue—and remains unable to handle indels due to fixed sequence-length constraint. Consequently, autoregressive models typically remain the preferred choice for likelihood-based analyses of mutations.</p> <p>To mitigate computational costs, the authors propose approximating PLL via a single inference, leveraging a concept they termed <strong>mask-consistent models</strong>. Mask-consistent models, exemplified by BERT with its fixed masking probability (15% total: 80% masked, 10% substituted, 10% unchanged), satisfy \begin{equation} P(y_i = x_i | \psi = 1, x) = P(y_i = x_i | x_{\setminus i}). \end{equation} This equivalence allows PLL to be computed efficiently within a single inference pass. Therefore, the probability \begin{equation} P(y_i = x_i | x_{\setminus i}, \theta) \end{equation} in PLL(x) can be redefined using a fixed masking schedule:</p> <p>\begin{equation} \begin{split} P(y_i = x_i | x_{\setminus i}, \theta) = \frac{\alpha + \beta}{\alpha} P(y_i = x_i | x, \theta) - \frac{\beta}{\alpha}, \end{split} \end{equation}</p> <p>Here, $\alpha$ is the probability of random substitution, and $\beta$ is the probability of leaving the residue unchanged. (<em>e.g.,</em> BERT employs $\alpha$ = 0.1 and $\beta$ = 0.1). The authors utilize this single-inference PLL approach as a computationally efficient replacement for standard PLL, enabling large-scale comparisons among various pLMs and examining the relationship between task performance and sequence likelihood. They validate this approximation by demonstrating a close match between PLL and single-inference PLL across proteins from the different biological kingdoms.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <img class="invert" style="max-width: 50%; height: auto;" src="/assets/img/2025-04-28-20243287/single_pll.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms of life. </figcaption> </figure> <h3 id="preference-for-wt-sequence-determines-mutation-effect">Preference for WT sequence determines mutation effect</h3> <p>Throughout their investigation, the authors primarily experiment with two prominent pLM models: <strong>ProGen2</strong> (autoregressive) and <strong>ESM2</strong> (BERT-based). They first explore how model size impacts the correlation between sequence likelihood and zero-shot fitness prediction. For smaller models (8M and 35M parameters), downstream predictive accuracy is largely dependent on the WT sequence likelihood. These smaller models exhibit better performance on sequences closely resembling their training set, as their predictive capabilities are constrained by limited representational power.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <img class="invert" style="max-width: 30%; height: auto;" src="/assets/img/2025-04-28-20243287/small_plms.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Model parameter counts vs. mean Spearman correlation on DMS datasets across models. </figcaption> </figure> <p><strong>However, how about larger models?</strong> The authors observe a distinct trend for the larger models (150M, 650M, 3B, 15B parameters). These larger models have been overfitted thus over-preferring particular sequences.</p> <p>Consequently, their performance declines as the WT sequence likelihood increases, indicating a tendency toward memorization rather than generalization. Figure 7 better showcases this concept, visualizing how the model’s plausible mutational space initially converges toward high fitness space but eventually diminishes.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <iframe src="/assets/html/2025-04-28-20243287/space.html#loop1" width="300" height="300" style="border: 0; display: block;" scrolling="no"></iframe> <figcaption class="caption" style="text-align: center;"> The ability of pLMs for economic protein maturation is dependent on more than nature’s plausible mutations. <d-cite key="hie2024"></d-cite>. </figcaption> </figure> <h3 id="why-does-pll-correlate-with-downstream-model-performance">Why does PLL correlate with downstream model performance?</h3> <p>To clarify the observed correlation between PLL and downstream predictive accuracy, the authors introduce the concept of <strong>influence functions</strong>, a framework for quantifying how individual training samples affect model predictions. <em>i.e., ’how much did a specific training data point affect the model’s prediction?’</em></p> <p>Given a training dataset $D$ composed of $N$ samples, $z_i$, the model’s optimal parameters $\theta^{\ast}$ are obtained by minimizing the loss function $L$</p> <p>\begin{equation} \begin{split} \theta^{\ast} \;=\; \underset{\theta^{\ast}}{\mathrm{arg\,min}}\;J(D, \theta) \;=\; \underset{\theta^{\ast}}{\mathrm{arg\,min}}\;\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}\bigl(z_i, \, \theta\bigr) \end{split} \end{equation}</p> <p>To specifically assess the influence of a single data point, $z_m$ when $i = m$, the authors introduce a perturbation parameter $\epsilon$, representing the weighted contribution of the selected datum to the overall loss.</p> <p>\begin{equation} \begin{split} \theta^{\ast}(\epsilon) \;=\; \underset{\theta^{\ast}}{\mathrm{arg\,min}}\;\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}\bigl(z_i, \, \theta\bigr) + \epsilon\mathcal{L}\bigl(z_m, \theta\bigr) \end{split} \end{equation}</p> <p>Using a first‐order Taylor expansion around $\epsilon=0$, the perturbed parameter set $\theta^{\ast}(\epsilon)$ can be written as</p> \[\theta^{\ast}(\epsilon) = \theta^{\ast}(0) + \epsilon\,\left.\frac{d\,\theta^{\ast}(\epsilon)}{d\epsilon}\right|_{\epsilon=0}.\] <p>Here,</p> \[\left.\frac{d\,\theta^{\ast}(\epsilon)}{d\epsilon}\right|_{\epsilon=0}\] <p>captures the influence of the datum $z_m$ on $\theta^{\ast}$.</p> <p>Applying the Implicit Function Theorem and chain rule, we derive</p> \[\begin{split} \left. \frac{d\,\theta^{\ast}(\epsilon)}{d\epsilon} \right|_{\epsilon=0} \;=\; -\,H^{-1}\,\nabla_{\theta}\mathcal{L}\bigl(z_{m},\,\theta^{\ast}\bigr). \end{split}\] <p>Here, the Hessian matrix, $H$, represents how sensitive the model is to parameter changes, and the latter term denotes the gradient of loss from the training point, $z_m$.</p> <p>To evaluate how changes in $z_m$ impact model predictions, the authors introduce the function $f$ to be a function for the likelihood of the test data</p> \[\begin{split} \left.\frac{d\,f\bigl(\theta^{\ast}(\epsilon)\bigr)}{d\epsilon}\right|_{\epsilon=0} &amp;= \left.\nabla_{\theta}f\bigl(\theta^{\ast}(\epsilon)\bigr)^{T} \;\cdot\;\frac{d\,\theta^{\ast}(\epsilon)}{d\epsilon}\right|_{\epsilon=0} \\ &amp;= -\,\nabla_{\theta}f\bigl(\theta^{\ast}\bigr)^{T} \,H^{-1}\, \nabla_{\theta}\mathcal{L}\bigl(z_{m},\,\theta^{\ast}\bigr)\,. \end{split}\] <h3 id="influence-diminishes-with-edit-distance">Influence diminishes with edit distance</h3> <p>One can easily hypothesize that if there are more homologous samples in the training set, then the model would perform better on the test dataset. Indeed, smaller models exhibit biases toward such homologs due to their limited representational capacities. Similarly, the model’s performance would diminish as the divergence between training and test sequences increases.</p> <p>Using the previously derived influence function, empirical analyses support this intuition—model influence on prediction notably decreases with the increase in the Hamming distance, reinforcing that the effectiveness of downstream predictions, like $\Delta\Delta G$ of mutation effect, is heavily reliant on the composition and filtering of the training data.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <img class="invert" style="max-width: 50%; height: auto;" src="/assets/img/2025-04-28-20243287/influence.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Influence on likelihood decreases as hamming distance increases. </figcaption> </figure> <p>Consequently, accurate predictions of pLMs are significantly influenced by preprocessing and selection of the training data.</p> <h3 id="a-novel-robust-fine-tuning-strategy">A novel, robust fine-tuning strategy</h3> <p>Intuitively, training pLMs on a diverse, broadly sampled dataset should enhance model generalization. However, this expectation in general contrasts with empirical observations, where typically only a small subset of training sequences strongly influences the resultant sequence likelihood predictions. A majority of sequences exert minimal or negligible impact, thus introducing substantial bias if not addressed explicitly.</p> <figure style="display: flex; flex-direction: column; align-items: center;"> <img class="invert" style="max-width: 50%; height: auto;" src="/assets/img/2025-04-28-20243287/power_tail.png" frameborder="0" scrolling="no"> <figcaption class="caption"> pLM influence tails exhibits a power law relationship. </figcaption> </figure> <p>Therefore, relying solely on a naïve fine-tuning procedure may fail to correct underlying biases, especially for downstream tasks involving underrepresented sequences.</p> <p>To counteract these biases, the authors propose a straightforward yet robust fine-tuning pipeline:</p> <ol> <li>Evaluate PLL scores using a pretrained pLM.</li> <li>If the PLL score is significantly low, filter out homologs from the fine-tuning dataset, and then implement Evo-tuning.</li> <li>Otherwise, fine-tune your-selected pLM directly on the available data. Applying this method to the ESM2 model reveals tangible performance improvements for underrepresented proteins as fine-tuning epochs increase. However, there exists a critical threshold, $\epsilon$, beyond which additional fine-tuning yields diminishing returns or even harms model performance.</li> </ol> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/naive_tuning.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Wild type likelihoods are predictive of finetuning success. (a) The change in Spearman correlation with increasing epoch. (b) The cumulative gain in performance for studies below likelihood $\epsilon$. </figcaption> </figure> <p>Empirical results indicate an optimal fine-tuning threshold at $\epsilon = -1.4$ that it confers ESM-2 650M model the maximum performance improvement compared to its baseline and naïvely evo-tuned counterparts. Importantly, this fine-tuning strategy generalizes effectively across diverse downstream tasks beyond zero-shot mutation-effect prediction.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/finetuning.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Comparing finetuned ESM-2 650M to state-of-the-art pLMs. </figcaption> </figure> <h2 id="concluding-remark">Concluding Remark</h2> <p>Recent advancements in molecular and protein representations—particularly the development of Ewald-based long-range message passing and the fine-tuning pipeline for pLMs—demonstrate the significance of borrowing and integrating ideas from distinct scientific disciplines. Both papers clearly illustrate how critical it is to recognize and carefully address domain-specific limitations: EMP, while powerful, has yet to be broadly validated on force-related downstream tasks; similarly, zero-shot fitness score function inherently suffers from modeling indels and combinatorial effects of multiple mutations.</p> <p>Furthermore, the EMP framework also aligns closely with a prominent hybrid message-passing model, GraphGPS <d-cite key="rampášek2023recipegeneralpowerfulscalable"></d-cite>. GraphGPS processes local aggregation and global interactions in parallel across layers via a standard GNN and transformer-based self-attention. Both EMP and GraphGPS emphasize the importance of simultaneously capturing short-range and long-range interactions in accomplishing the complete complexity of molecular interactions.</p> <p>Furthermore, this principle of combining local and global interaction is also echoed in other recent studies. For instance, the Neural Atom architecture addresses a similar conceptual challenge with Ewald Summation principle <d-cite key="li2024neuralatomspropagatinglongrange"></d-cite>. Neural Atom explicitly projects atomic features into reciprocal space via cross-attention between initial atomic features and randomly initialized Neural Atom features. The Reciprocal features then conduct self-attention with each other, performing the long-range updates and then are projected back into the real space using the attention weights learned during the cross attention. While its Ewald-based approach is conceptually simpler than the kernelized EMP, Neural Atom nonetheless embodies the same core intuition—balancing fine-grained local structural details with global-range influences.</p> <p>Moreover, recent comparative analyses further highlight the subtle complexities involved in encoding biomolecules <d-cite key="adamczyk2025molecularfingerprintsstrongmodels"></d-cite>. It suggests that traditional cheminformatic descriptors, specifically Extended-Connectivity Fingerprints (ECFP) coupled with a Light Gradient Boosting Machine (lightGBM) can outperform GNNs with explicit long-range interactions and pLM-based encodings in antimicrobial peptide classification tasks. This indicates that classical finger-print based methods—implicitly analogous to shallow GNNs—can also provide robust molecular representations that, under specific circumstances, rival deep learning counterparts, serving as a caution against over-reliance on complex deep learning architectures.</p> <p>In conclusion, these recent developments confer a central insight: no single representation strategy—a pure local message passing, a graph transformer, or solely statistical language modeling—can ultimately capture the complexity in the protein sequences. Therefore, future likely belongs to carefully engineered hybrid architectures and methodologies that can harmoniously combine statistical rigor, local geometric accuracy, and global context awareness. Continued exploration of approaches like GraphGPS, EMP, Neural Atom, and statistical diagnostics exploiting the notion of preference and influence will guide us toward richer and more robust molecular representations, consequently driving progress in real-world applications including protein design and drug discovery.</p> </body></html>