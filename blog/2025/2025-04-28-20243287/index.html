<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="motivation-for-this-review">Motivation for this review</h2> <p>Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology. Early encodings—one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors—laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function. In response, protein language models (PLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids—and extending them to atom-level resolution—continues to pose a significant challenge. Meanwhile, graph neural networks (GNN) convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules. Similarly, how to effectively leverage multi-scale PLMs into a particular protein domain remains elusive.</p> <p>With this blog post, we examine two recent studies designed to close these gaps. First, we revisit Ewald Summation—long established in molecular dynamics for treating infinite periodic systems—and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs. Then, we explore a fine-tuning paradigm for PLMs that how we can strategically adapt large, pretrained PLMs into specific protein domain without compromising their performance. Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability. Whether it’s importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained PLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential—and challenging those assumptions often leads to the deepest insights.</p> <h2 id="ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</h2> <h3 id="why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</h3> <p>In standard Message Passing Neural Networks (MPNNs) for molecular systems, each atom’s representation is updated by aggregating information from the neighbors within a <strong>fixed distance cutoff</strong>. While this effectively captures local chemical interactions especially at the Dalton scale, it systematically neglects long-range forces such as electrostatics, dispersion, and allosteric effects in larger molecules (often kilodalton to megadalton scale).</p> <p>This locality bias in large graphs gives to over-squashing problem of GNNs <d-cite key="alon2021bottleneck"></d-cite>. Information from distant nodes must traverse many intermediate aggregations, compressing rich signals into narrow bottlenecks and degrading the network’s ability to learn non-local dependencies. Therefore, multiple strategies, including sensitivity analysis and graph-rewiring, have been addressed to solve this problem in long-range interaction between a source and target vertex.</p> <p>In particular, we will explore how the <strong>Ewald Summation</strong> principle can be reformulated as an agnostic message-passing module to directly mitigate over-squashing and endow geometric GNNs with a true global context.</p> <h3 id="what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</h3> <p>Before we define Ewald Summation, we must first introduce the notion of <strong>Periodic Boundary Conditions</strong> (PBC) and <strong>Supercell lattice</strong>. Under PBCs, a finite simulation box is conceptually tiled infinitely in every direction (see Figure 1). This wrapping ensures every atom experience a uniform environment and eliminates any edge artifacts. The Supercell lattice specifies the integer combinations of the box’s basis vectors that generate all image cells (the replicas)—atom embeddings in the primary cell thus can be replicated implicitly across these translations when computing interactions.</p> <p>Without PBCs, a naïve sum of Coulomb potential of ions with distance cutoff over an infinite lattice is only conditionally convergent and thus computationally infeasible. (<em>e.g.</em> the long tail problem from the inversely proportional to the distance). The solution lies in recognizing that the direct lattice can be approximated using reciprocal lattice in Fourier space with a frequency cutoff. Ewald Summation exploits this dual structure by decomposing the problematic infinite sum ($\Phi(r)$) into two rapidly, absolutely convergent series: short-range interaction ($\Phi^{sr}(r)$) and long-range interaction ($\Phi^{lr}(r)$), which can be expressed in the Fourier space as $\hat\Phi^{lr}(||\dot||)$.</p> <p>\begin{equation} \begin{split} \Phi(r) = \Phi_{\mathrm{sr}}(r) + \Phi_{\mathrm{lr}}(r), \ <br> r = \lVert \mathbf{x}_i - \mathbf{x}_j\rVert. \end{split} \end{equation}</p> <h3 id="ewald-message-passing">Ewald Message Passing</h3> <p>Authors of this paper adopt this exact decomposition of $\Phi(r)$ as the foundation of the Ewald Message Passing (EMP) module, which can serve as an augmentation on top of existing MPNN architectures. The short-range convolution, $\Phi^{sr}(r)$, makes use of the continuous-filter convolution employed by any geometric GNN that implements distance-dependent message function (<em>e.g.</em> SchNet, PaiNN, DimeNet++, GemNet-T). Here, an atom gathers information from all neighbors within a prescribed Euclidean cutoff by applying a learnable radial filter to inter-atomic distances. This preserves the model’s capacity to capture local chemical phenomena without any explicit modification into the pre-existing GNN architecture.</p> <p>To implement the long-range stream in a graph context, they first define a <strong>structure factor</strong>, $s_{k}$, a notion from crystallography, as a Fourier space representation of atom embeddings, which defines a measure of how node embeddings, $h_{j}$, are collectively scatter at frequency <em>k</em> (<em>i.e.</em> a weighted sum over <em>all</em> supercell embeddings). To obtain $s_{k}$, $h_{j}$’s are projected to the reciprocal space by computing over a fixed, learnable set of wavevectors {<em>k</em>}</p> <p>\begin{equation} \begin{split} s_{\mathbf{k}} &amp;= \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}^{T}\mathbf{x}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Once in the reciprocal space, $\Lambda^{\prime}$, each mode is modulated by a learnable frequency filter, $\hat{\Phi}_{\mathrm{lr}}(||\mathbf{k}||)$, yielding</p> <p>\begin{equation} \begin{split} \hat{M}^{\mathrm{lr}}(\mathbf{x_i}) &amp;= s_{\mathbf{k}}\cdot\hat{\Phi}_{\mathrm{lr}}\bigl(||\mathbf{k}||\bigr)\,. \end{split} \end{equation}</p> <p>Next, $s_{k}$ is projected back into the real position space through inverse transformation</p> <p>\begin{equation} \begin{split} M_i^{\mathrm{lr}} &amp;= \sum_{\mathbf{k}\in\Lambda^{\prime}} \hat{M_k}^{lr}\;exp({\,i\,\mathbf{k}\cdot \mathbf{x}_i})\,, \end{split} \end{equation}</p> <p>Finally, $M^{lr}$ is added to $M^{sr}$, forming a complete update cycle for a single layer of GNN-EMP architecture</p> <p>\begin{equation} \begin{split} h_i^{(\ell+1)} &amp;= \frac{1}{\sqrt{3}}\Bigl(h_i^{(\ell)} + f_{\mathrm{upd}}^{\mathrm{sr}}\bigl(M_i^{\mathrm{sr}}\bigr) + f_{\mathrm{upd}}^{\mathrm{lr}}\bigl(M_i^{\mathrm{lr}}\bigr)\Bigr)\,. \end{split} \end{equation} Authors employ two mode-generation schemes depending on whether the system is periodic or aperiodic. For an aperiodic case, there is no reciprocal lattice $\Lambda^{\prime}$, so the structure factor, $s_k$ map is first regarded as a continuous function from $\mathbb{R}^3$ to the D-dimensional complex numbers.</p> <p>\begin{equation} \begin{split} s(k): \mathbb{R}^3 &amp;\to \mathbb{C}^d,<br> \mathbf{k} &amp;\mapsto \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}\cdot\mathbf{r}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Because the real-space supercell size and the spacing of admissible k-vectors are inversely related, this field must be sampled on a finite grid. Therefore, they place a uniform voxel grid of <em>k</em> wave-vectors and replace all continuous functions by its voxel averages. To prevent aliasing and to guarantee rotational invariance, the grid is defined in a co-rotating frame obtained from principal component analysis (PCA) of the atomic point cloud: the three PCA eigen-vectors (ordered by descending eigen-value) form an orthonormal basis that rotates with the molecule. Rotating the molecule in real space then rotates the voxel grid in the reciprocal space, keeping the sampled modes and their learned filter unaffected by the global orientation.</p> <p>Similarly, <strong>frequency filters</strong>, $\hat\Phi(||\cdot||)$, are parameterized differently in the two settings. In aperiodic cases, they employ the same radial-basis parametrization used in ScheNet: $\hat\Phi(||k||)$ is expressed as a linear combination of Gaussian radial basis functions whose centers lie within k-cutoff radius, $c_k$ in Fourier space. Because the filter depends on the magnitude of $||k||$, it is rotation-invariant once the molecule has been aligned to its PCA frame. However, for periodic systems, a radial envelope would be overly restrictive; the periodic boundary conditions carry valuable geometric information. Instead, the filter is non-radial: a separate complex coefficient is learned for every point-symmetry orbit in the reciprocal lattice $\Lambda^{\prime}$. Because the reciprocal lattice co-rotates with the supercell lattice, the learned weights remain rotation-invariant, even though the filter is no longer radial.</p> <h3 id="results-on-long-range-interaction">Results on long-range interaction</h3> <p>The authors evaluate EMP module on two public data sets with contrasting geometry:</p> <ul> <li>OC20 — 133M surface-adsorbate structures from density functional theory (DFT) relaxations, ~50-200 atoms each</li> <li>OE62 — 62k organic molecules, energies include D3 dispersion, ~50-100 atoms each</li> </ul> <p>As baselines, four geometric GNN models are considered: the node-based models SchNet and PaiNN, and the edge-based models DimeNet++ and GemNet-T. Each EMP variant is parameter-matched to its baseline by reducing the hidden size so that both networks contain the same total number of trainable parameters. Energy mean absolute error (EMAE) is used as the evaluation metric.</p> <p>For both datasets, adding EMP consistently lowers EMAE on all four baseline models. However, for OE62, the edge-based models gain trivial improvement because a 12 angstrom radius already saturates nearly every graph in the dataset; therefore, long-range information processed by EMP is partly redundant.</p> <p>To test whether the improvement stems from capturing long-rage interaction rather than from an architectural artefact, the authors compare EMP to an explicit DFT-D3 correction on OE62. Figure 3 shows a strong positive correlation between D3 energy magnitude and the EMAE gain delivered by EMP. Moreover, for SchNet and DimeNet++ the learned correction outperforms the hand-crafted D3 term when both are applied to the same baseline. This indicates that EMP is not only learning Van Dal Waals force without any hand-crafted correction but also enriching embeddings with auxiliary factors as well.</p> <p>Limitation Compare to energy, forces are more dominated by the high wavelength terms rather than low wavelength terms. This is what not addressed by the EMP, because EMP focuses on the low wavelength terms within the prescribed frequency cutoff. Therefore, EMP is more of an energy correction scheme.</p> <h2 id="protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</h2> <h2 id="discussions">Discussions</h2> <h2 id="conclusion">Conclusion</h2> <p>Future work I have not discussed in this post, but several other papers also tried to tackle the same long range problem, one with naively relating attention with Ewald principle (Neural Atom, ICLR2024) and …</p> </body></html>