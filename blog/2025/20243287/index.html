<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI810 Blog Post (20243287) | Dongje Moon </title> <meta name="author" content="Dongje Moon"> <meta name="description" content="This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dongjemoon.github.io/blog/2025/20243287/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.preamble{counter-reset:all-theorems all-figures all-definitions}figure>figcaption:not(:has(p)),figure>figcaption>p:first-child{counter-increment:all-figures;&::before{display:inline;content:"Figure " counter(all-figures) ". "}}.fig-push{counter-increment:all-figures;width:0;visibility:hidden}.fig-pop{counter-increment:all-figures -1;width:0;visibility:hidden}.ref-lastfig::after{content:counter(all-figures)}figcaption,.figcaption{color:var(--global-text-color)!important}summary{color:var(--global-text-color)!important}html[data-theme="dark"]{.grid-custom1 img{padding-top:1em;background:white}mjx-container{color:var(--global-text-color)}}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "AI810 Blog Post (20243287)",
            "description": "This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025).",
            "published": "April 28, 2025",
            "authors": [
              
              {
                "author": "Dongje Moon",
                "authorURL": "https://dongjemoon.github.io/",
                "affiliations": [
                  {
                    "name": "KAIST",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dongje</span> Moon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>AI810 Blog Post (20243287)</h1> <p>This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivation-for-this-review">Motivation for this review</a> </div> <div> <a href="#ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</a> </div> <ul> <li> <a href="#why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</a> </li> <li> <a href="#what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</a> </li> <li> <a href="#ewald-message-passing">Ewald Message Passing</a> </li> <li> <a href="#results-on-long-range-interaction">Results on long-range interaction</a> </li> </ul> <div> <a href="#limitation">Limitation</a> </div> <div> <a href="#protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</a> </div> <ul> <li> <a href="#recent-works-reveal-limitations-of-sota-plms">Recent works reveal limitations of SoTA PLMs</a> </li> <li> <a href="#what-is-preference-why-preference">What is preference? Why preference?</a> </li> <li> <a href="#single-inference-pseudo-log-likelihood">Single-inference pseudo log likelihood</a> </li> <li> <a href="#influence-diminishes-with-edit-distance">Influence diminishes with edit distance</a> </li> <li> <a href="#a-novel-fine-tuning-pipeline-based-on-pll">A novel fine-tuning pipeline based on PLL</a> </li> </ul> <div> <a href="#discussions">Discussions</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="motivation-for-this-review">Motivation for this review</h2> <p>Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology. Early encodings—one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors—laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function. In response, protein language models (PLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids—and extending them to atom-level resolution—continues to pose a significant challenge. Meanwhile, graph neural networks (GNN) convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules. Similarly, how to effectively leverage multi-scale PLMs into a particular protein domain remains elusive.</p> <p>With this blog post, we examine two recent studies designed to close these gaps. First, we revisit Ewald Summation—long established in molecular dynamics for treating infinite periodic systems—and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs. Then, we explore a fine-tuning paradigm for PLMs that how we can strategically adapt large, pretrained PLMs into specific protein domain without compromising their performance. Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability. Whether it’s importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained PLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential—and challenging those assumptions often leads to the deepest insights.</p> <h2 id="ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</h2> <h3 id="why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</h3> <p>In standard Message Passing Neural Networks (MPNNs) for molecular systems, each atom’s representation is updated by aggregating information from the neighbors within a <strong>fixed distance cutoff</strong>. While this effectively captures local chemical interactions especially at the Dalton scale, it systematically neglects long-range forces such as electrostatics, dispersion, and allosteric effects in larger molecules (often kilodalton to megadalton scale).</p> <p>This locality bias in large graphs gives to over-squashing problem of GNNs <d-cite key="alon2021bottleneck"></d-cite>. Information from distant nodes must traverse many intermediate aggregations, compressing rich signals into narrow bottlenecks and degrading the network’s ability to learn non-local dependencies. Therefore, multiple strategies, including sensitivity analysis and graph-rewiring, have been addressed to solve this problem in long-range interaction between a source and target vertex.</p> <p>In particular, we will explore how the <strong>Ewald Summation</strong> principle can be reformulated as an agnostic message-passing module to directly mitigate over-squashing and endow geometric GNNs with a true global context.</p> <h3 id="what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</h3> <p>Before we define Ewald Summation, we must first introduce the notion of <strong>Periodic Boundary Conditions</strong> (PBC) and <strong>Supercell lattice</strong>. Under PBCs, a finite simulation box is conceptually tiled infinitely in every direction (see Figure 1). This wrapping ensures every atom experience a uniform environment and eliminates any edge artifacts. The Supercell lattice specifies the integer combinations of the box’s basis vectors that generate all image cells (the replicas)—atom embeddings in the primary cell thus can be replicated implicitly across these translations when computing interactions.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/pbc.webp" frameborder="0" scrolling="no"> <figcaption class="caption"> Schematic representation of periodic boundary conditions </figcaption> </figure> <p>Without PBCs, a naïve sum of Coulomb potential of ions with distance cutoff over an infinite lattice is only conditionally convergent and thus computationally infeasible. (<em>e.g.</em> the long tail problem from the inversely proportional to the distance). The solution lies in recognizing that the direct lattice can be approximated using reciprocal lattice in Fourier space with a frequency cutoff. Ewald Summation exploits this dual structure by decomposing the problematic infinite sum ($\Phi(r)$) into two rapidly, absolutely convergent series: short-range interaction ($\Phi^{sr}(r)$) and long-range interaction ($\Phi^{lr}(r)$), which can be expressed in the Fourier space as $\hat\Phi^{lr}(\left| \cdot \right|)$.</p> <p>\begin{equation} \begin{split} \Phi(r) = \Phi_{\mathrm{sr}}(r) + \Phi_{\mathrm{lr}}(r), \ <br> r = \lVert \mathbf{x}_i - \mathbf{x}_j\rVert. \end{split} \end{equation}</p> <h3 id="ewald-message-passing">Ewald Message Passing</h3> <p>Authors of this paper adopt this exact decomposition of $\Phi(r)$ as the foundation of the Ewald Message Passing (EMP) module, which can serve as an augmentation on top of existing MPNN architectures. The short-range convolution, $\Phi^{sr}(r)$, makes use of the continuous-filter convolution employed by any geometric GNN that implements distance-dependent message function (<em>e.g.</em> SchNet, PaiNN, DimeNet++, GemNet-T). Here, an atom gathers information from all neighbors within a prescribed Euclidean cutoff by applying a learnable radial filter to inter-atomic distances. This preserves the model’s capacity to capture local chemical phenomena without any explicit modification into the pre-existing GNN architecture.</p> <p>To implement the long-range stream in a graph context, they first define a <strong>structure factor</strong>, $s_{k}$, a notion from crystallography, as a Fourier space representation of atom embeddings, which defines a measure of how node embeddings, $h_{j}$, are collectively scatter at frequency <em>k</em> (<em>i.e.</em> a weighted sum over <em>all</em> supercell embeddings).</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/ewald_sum.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Three main process of Ewald Message Passing: first, group of atom embeddings are projected to Fourier space as structure factor embeddings. Next, the embeddings are multiplied k-cutoff-wise with learned filter values. Lastly, the reciprocal embeddings are projected back into real space. </figcaption> </figure> <p>To obtain $s_{k}$, $h_{j}$’s are projected to the reciprocal space by computing over a fixed, learnable set of wavevectors {<em>k</em>}</p> <p>\begin{equation} \begin{split} s_{\mathbf{k}} &amp;= \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}^{T}\mathbf{x}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Once in the reciprocal space, $\Lambda^{\prime}$, each mode is modulated by a learnable frequency filter, $\hat{\Phi}_{\mathrm{lr}}(||\mathbf{k}||)$, yielding</p> <p>\begin{equation} \begin{split} \hat{M}^{\mathrm{lr}}(\mathbf{x_i}) &amp;= s_{\mathbf{k}}\cdot\hat{\Phi}_{\mathrm{lr}}\bigl(||\mathbf{k}||\bigr)\,. \end{split} \end{equation}</p> <p>Next, $s_{k}$ is projected back into the real position space through inverse transformation</p> <p>\begin{equation} \begin{split} M_i^{\mathrm{lr}} &amp;= \sum_{\mathbf{k}\in\Lambda^{\prime}} \hat{M_k}^{lr}\;exp({\,i\,\mathbf{k}\cdot \mathbf{x}_i})\,, \end{split} \end{equation}</p> <p>Finally, $M^{lr}$ is added to $M^{sr}$, forming a complete update cycle for a single layer of GNN-EMP architecture</p> <p>\begin{equation} \begin{split} h_i^{(\ell+1)} &amp;= \frac{1}{\sqrt{3}}\Bigl(h_i^{(\ell)} + f_{\mathrm{upd}}^{\mathrm{sr}}\bigl(M_i^{\mathrm{sr}}\bigr) + f_{\mathrm{upd}}^{\mathrm{lr}}\bigl(M_i^{\mathrm{lr}}\bigr)\Bigr)\,. \end{split} \end{equation} Authors employ two mode-generation schemes depending on whether the system is periodic or aperiodic. For an aperiodic case, there is no reciprocal lattice $\Lambda^{\prime}$, so the structure factor, $s_k$ map is first regarded as a continuous function from $\mathbb{R}^3$ to the D-dimensional complex numbers.</p> <p>\begin{equation} \begin{split} s(k): \mathbb{R}^3 &amp;\to \mathbb{C}^d,<br> \mathbf{k} &amp;\mapsto \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}\cdot\mathbf{r}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Because the real-space supercell size and the spacing of admissible k-vectors are inversely related, this field must be sampled on a finite grid. Therefore, they place a uniform voxel grid of <em>k</em> wave-vectors and replace all continuous functions by its voxel averages. To prevent aliasing and to guarantee rotational invariance, the grid is defined in a co-rotating frame obtained from principal component analysis (PCA) of the atomic point cloud: the three PCA eigen-vectors (ordered by descending eigen-value) form an orthonormal basis that rotates with the molecule. Rotating the molecule in real space then rotates the voxel grid in the reciprocal space, keeping the sampled modes and their learned filter unaffected by the global orientation.</p> <p>Similarly, <strong>frequency filters</strong>, $\hat\Phi(||\cdot||)$, are parameterized differently in the two settings. In aperiodic cases, they employ the same radial-basis parametrization used in ScheNet: $\hat\Phi(||k||)$ is expressed as a linear combination of Gaussian radial basis functions whose centers lie within k-cutoff radius, $c_k$ in Fourier space. Because the filter depends on the magnitude of $||k||$, it is rotation-invariant once the molecule has been aligned to its PCA frame. However, for periodic systems, a radial envelope would be overly restrictive; the periodic boundary conditions carry valuable geometric information. Instead, the filter is non-radial: a separate complex coefficient is learned for every point-symmetry orbit in the reciprocal lattice $\Lambda^{\prime}$. Because the reciprocal lattice co-rotates with the supercell lattice, the learned weights remain rotation-invariant, even though the filter is no longer radial.</p> <h3 id="results-on-long-range-interaction">Results on long-range interaction</h3> <p>The authors evaluate EMP module on two public data sets with contrasting geometry:</p> <ul> <li>OC20 — 133M surface-adsorbate structures from density functional theory (DFT) relaxations, ~50-200 atoms each</li> <li>OE62 — 62k organic molecules, energies include D3 dispersion, ~50-100 atoms each</li> </ul> <p>As baselines, four geometric GNN models are considered: the node-based models SchNet and PaiNN, and the edge-based models DimeNet++ and GemNet-T. Each EMP variant is parameter-matched to its baseline by reducing the hidden size so that both networks contain the same total number of trainable parameters. Energy mean absolute error (EMAE) is used as the evaluation metric.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/ewald_result1.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Cost vs. EMAE on OE62 and OC20, respectively. </figcaption> </figure> <p>For both datasets, adding EMP consistently lowers EMAE on all four baseline models. However, for OE62, the edge-based models gain trivial improvement because a 12$\text{\AA}$ radius already saturates nearly every graph in the dataset; therefore, long-range information processed by EMP is partly redundant.</p> <figure class="l-page" style="display: flex; justify-content: center; gap: 1rem;"> <img src="/assets/img/2025-04-28-20243287/ewald_result2.png" style="width: 50%; height: auto;"> <img src="/assets/img/2025-04-28-20243287/ewald_result3.png" style="width: 50%; height: auto;"> <figcaption class="caption" style="text-align: center; width: 100%;"> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Ewald-MP vs. DFT-D3 correction for EMAE on OE62.
</code></pre></div> </div> </figcaption> </figure> <p>To test whether the improvement stems from capturing long-rage interaction rather than from an architectural artefact, the authors compare EMP to an explicit DFT-D3 correction on OE62. Figure 3 shows a strong positive correlation between D3 energy magnitude and the EMAE gain delivered by EMP. Moreover, for SchNet and DimeNet++ the learned correction outperforms the hand-crafted D3 term when both are applied to the same baseline. This indicates that EMP is not only learning Van Dal Waals force without any hand-crafted correction but also enriching embeddings with auxiliary factors as well.</p> <h3 id="limitation">Limitation</h3> <p>While EMP introduces a compelling paradigm to model long-range interactions in geometric GNNs, it also comes with several limitations. One notable limitation of the current formulation is that the model performance succeeds only on energy prediction, while only subtle improvement observed in force MAE. This is important because molecular dynamics or structure optimization depends heavily on forces. However, since forces are energy <strong>gradients</strong> more dominated by fast-changing short wavelength terms and EMP focuses on the long wavelength terms within the prescribed frequency cutoff, \begin{equation} \begin{split} \vec{F}(\vec{x_i}) = -\vec{\Delta}V(\vec{x_i}) = -\sum_{\vec{k}}i\vec{k}\hat{V_{\vec{k}}}exp(i\vec{k}^T\vec{x}) \end{split} \end{equation} EMP becomes more of an energy correction scheme rather than force correction scheme.</p> <p>Another limitation of the current EMP framework lies in the reliance on <strong>fixed input geometries</strong>. The long-range interactions, computed via structure factor embeddings in the reciprocal space, are determined only once initially based on the input atomic positions and do not adapt throughout the message passing process. This static formulation contrasts with equivariant architectures like PaiNN or GemNet-T, where both scalar and vector features evolve jointly over layers in response to updated representations. As such, EMP does not adjust the k-space contributions in response to intermediate feature updates, which may limit the model’s capacity to learn feedback effects between short- and long-range interactions during inference.</p> <p>Similarly, the implementation of EMP in the paper operates exclusively on atom type and atomic coordinate embeddings. It does not incorporate vectorial features (<em>e.g.,</em> dipoles, angular momenta) or multi-modal information (<em>e.g.,</em> electronic density, spectroscopy). Extending EMP to such modalities would require significant architectural changes, particularly to preserve equivariance and rotational consistency in the Fourier domain.</p> <h2 id="protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</h2> <h3 id="recent-works-reveal-limitations-of-sota-protein-language-models">Recent works reveal limitations of SoTA protein language models</h3> <p>PLMs, such as ESM, MSA Transformer, EVE, ProGen, and TranceptEVE, have shown great capacity in inferring evolutionary trajectories, guiding various downstream tasks including protein design and predicting zero-shot mutational effects. Despite their success, recent studies highlight critical limitations, revealing that the PLMs are significantly influenced by biases present in their training data—biases that do not necessarily reflect true evolutionary fitness.</p> <ul> <li>PLMs primarily model the underlying distribution of the training data rather than genuine biological fitness functions <d-cite key="weinstein2022"></d-cite>.</li> <li>Training data are often biased towards frequently observed sequences rather than evolutionarily advantageous ones <d-cite key="ding2024"></d-cite>. (Ding &amp; Steinhardt 2024)</li> <li>Benchmarks used for model evaluation sometimes overlap with sequences used for training PLM, inflating the resultant model performance <d-cite key="hermann2024"></d-cite>. In other words, a sequence’s evolutionary frequency does not inherently correlate with biological superiority. To address this discrepancy, the authors introduce the concept of <strong>preference</strong> and <strong>influence</strong> to systematically investigate PLM performance. <h3 id="what-is-preference-why-preference">What is preference? Why preference?</h3> <p>The notion of preference stems from Bradly-Terry model of preference, which quantifies the probability of selecting one item over another: \begin{equation} \begin{split} Pr(i &gt; j) = p_i / (p_i + p_j) \end{split} \end{equation} Where $p_i$ and $p_j$ denote Protein i and Protein j, respectively. Preference models are well-established in various fields, notably direct preference optimization (DPO) and Elo rating systems. Within the context of PLMs, the preference translates into the probability that Protein i is preferred over Protein j is proportional to their sequence likelihood score. Hence, PLM implicitly prioritize sequences they recognize as evolutionarily familiar or plausible. <strong>What is causing this sequence preference—model architecture or training data?</strong> Intuition suggests that training data composition strongly influences these preferences, as PLMs likely reflect biases inherent in their datasets rather acquiring genuine biological insights. The authors similarly hypothesize that observed sequence preferences originate primarily from the training data rather than intrinsic model properties. To explore this hypothesis, the authors focus their study on protein fitness prediction, specifically leveraging deep mutational scan (DMS) datasets. These datasets assess the relative fitness of numerous protein mutants, assigning wild type (WT) fitness as a baseline (fitness = 1), with mutants scoring greater or lesser values depending on their relative performance, where greater values mean better fitness. The widely-used zero-shot fitness scoring function compares mutants $x’$ against WT $x$ sequences as follows: \begin{equation} \begin{split} F(x^{\prime}, x) = \sum_{t \in T}\logP(y_t = x^{\prime}<em>t | x</em>{\ t}, \theta) - \logP(y_t = x_t | x_{\ t}, \theta) \end{split} \end{equation} While commonly employed and benchmarked against experimental fitness via Spearman correlations, this formula introduces two significant limitations:</p> </li> <li> <strong>Linear Additivity Assumption</strong>: It assumes independence of individual mutations by summing their log-likelihood differences. However, real-world proteins frequently exhibit epistasis, where mutation effects are interdependent and non-linear. Consequently, the formula fails to capture complex interactions between multiple mutations.</li> <li>Sequence Length Constraint: It requires $x$ and $x’$ to have the same sequence length and forces residue-to-residue alignment between WT and mutant sequences. Therefore, it excludes insertion and deletions (indel) mutations, which are naturally prevalent. Recognizing these constraints, the authors seek to understand the influence of training data on PLM-derived preferences while working within these formula-imposed boundaries.</li> </ul> <h3 id="single-inference-pseudo-log-likelihood">Single-inference pseudo log-likelihood</h3> <p>Unlike autoregressive language models, masked language models cannot compute joint likelihood naturally. To address this limitation, Wang &amp; Cho <d-cite key="wang2019"></d-cite> introduced pseudo-log-likelihood (PLL): \begin{equation} \begin{split} PLL(x) = 1/L\sum_{i=1}^{L} \logP(y_i = x_i | x_{\ i}, \theta) \end{split} \end{equation} However, this PLL formulation suffers from $O(L)$ forward passes—one per masked residue—and remains unable to handle indels due to fixed sequence-length constraint. Consequently, autoregressive models typically remain the preferred choice for likelihood-based analyses of mutations.</p> <h3 id="single-inference-pseudo-log-likelihood-1">Single-inference pseudo log likelihood</h3> <p>To mitigate computational costs, the authors propose approximating PLL via a single inference, leveraging a concept they termed <strong>mask-consistent models</strong>. Mask-consistent models, exemplified by BERT with its fixed masking probability (15% total: 80% masked, 10% substituted, 10% unchanged), satisfy: $P(y_i = x_i | \psi = 1, x) = P(y_i = x_i | x_{\ i})$. This equivalence allows PLL to be computed efficiently within a single inference pass. Therefore, the probability P(y_i = x_i | x_{\ i}, \theta) in PLL(x) can be redefined using a fixed masking schedule: \begin{equation} \begin{split} P(y_i = x_i | x_{\ i}, \theta) = {\alpha + \beta} / \alpha P(y_i = x_i | x, \theta) - \beta / \alpha, \end{split} \end{equation} Here, \alpha is the probability of random substitution, and \beta is the probability of leaving the residue unchanged. (<em>e.g.,</em> BERT employs \alpha = 0.1 and \beta = 0.1). The authors utilize this single-inference PLL approach as a computationally efficient replacement for standard PLL, enabling large-scale comparisons among various PLMs and examining the relationship between task performance and sequence likelihood. They validate this approximation by demonstrating a close match between PLL and single-inference PLL across proteins from the different biological kingdoms.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/single_pll.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms of life. </figcaption> </figure> <h3 id="preference-for-wt-sequence-determines-mutation-effect">Preference for WT sequence determines mutation effect</h3> <p>Throughout their investigation, the authors primarily compare two prominent PLM models: ProGen2 (autoregressive) and ESM2 (BERT-based). They first explore how model size impacts the correlation between sequence likelihood and zero-shot fitness prediction. For smaller models (8M and 35M parameters), downstream predictive accuracy is largely dependent on the WT sequence likelihood. These smaller models exhibit better performance on sequences closely resembling their training set, as their predictive capabilities are constrained by limited representational power.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/small_plms.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <p><strong>However, how about larger models?</strong> The authors observe a distinct trend for the larger models (150M, 650M, 3B, 15B parameters). These larger models have been overfitted thus over-preferring particular sequences. Consequently, their performance declines as the WT sequence likelihood increases, indicating a tendency toward memorization rather than generalization. Their visual representation reinforces this concept, illustrating how the model’s plausible mutational landscape initially converges toward beneficial mutations but eventually diminishes.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/html/2025-04-28-20243287/space.html" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <h3 id="why-does-pll-correlate-with-downstream-model-performance">Why does PLL correlate with downstream model performance?</h3> <p>To clarify the observed correlation between PLL and downstream predictive accuracy, the authors introduce the concept of <strong>influence functions</strong>, a framework for quantifying how individual training samples affect model predictions. <em>i.e., ’how much did a specific training data point affect the model’s prediction?’</em> Given a training dataset $D$ composed of $N$ samples, $z_i$, the model’s optimal parameters \theta^{<em>} are obtained by minimizing the loss function $L$: $\theta^{</em>} \;=\; \argmin_{\theta^{<em>}}\;J(D, \theta) \;=\; \argmin_{\theta^{</em>}}\;\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}\bigl(z_i, \, \theta\bigr)$. To specifically assess the influence of a single data point, $z_m$ when $i = m$, the authors introduce a perturbation parameter \epsilon, representing the weighted contribution of the selected datum to the overall loss. \begin{equation} \begin{split} \theta^{<em>}(\epsilon) \;=\; \argmin_{\theta^{</em>}}^{<em>}}\;\frac{1}{N}\sum_{i=1}^{N} \mathcal{L}\bigl(z_i, \, \theta\bigr) + \epsilon\mathcal{L}\bigl(z_m, \theta\bigr) \end{split} \end{equation} Using a first-order Taylor expansion around $\epsilon = 0$, the perturbed parameter set, $theta^{</em>}(\epsilon) can be expressed as: $theta^{<em>}(\epsilon) \;=\; theta^{</em>}(\epsilon) + \epsilon \cdot \frac{d\theta^{<em>}(\epsilon)}{d\epsilon}\;\left|\right._{\epsilon=0}$, where the derivative $\frac{d\theta^{</em>}(\epsilon)}{d\epsilon}\;\left|\right.<em>{\epsilon=0}$ captures the influence of the datum $z_m$ on $\theta^{*}$. Applying the Implicit Function Theorem and chain rule, we derive: \begin{equation} \begin{split} \frac{d\theta^{*}(\epsilon)}{d\epsilon}\;\left|\right.</em>{\epsilon=0} \;=\; -H^{-1}\nabla_{\theta}\mathcal{L}\bigl(z_m, \, \theta\bigr) \end{split} \end{equation} Here, the Hessian matrix, $H$, represents how sensitive the model is to parameter changes, and the latter term denotes the gradient of loss from the training point, $z_m$. To evaluate how changes in $z_m$ impact model predictions, the authors introduce the function $f$ to be a function for the likelihood of the test data: \begin{equation} \begin{split} \frac{df(\theta^{<em>}(\epsilon))}{d\epsilon}\;\left|\right._{\epsilon=0} \;=\; \nabla_{\theta}f(\theta^{</em>})^{T} \cdot \frac{d\theta^{<em>}(\epsilon)}{d\epsilon}\;\left|\right._{\epsilon=0} \;=\; -\nabla_{\theta}f(\theta^{</em>})^{T}H^{-1}\nabla_{\theta}\mathcal{L}\bigl(z_m, \, \theta\bigr) \end{split} \end{equation}</p> <h3 id="influence-diminishes-with-edit-distance">Influence diminishes with edit distance</h3> <p>One can easily hypothesize that if there are more homologous samples in the training set, then the model would perform better on the test dataset. Indeed, smaller models exhibit biases toward such homologs due to their limited representational capacities. Similarly, the model’s performance would diminish as the divergence between training and test sequences increases. Using the previously derived influence function, empirical analyses support this intuition—model influence on prediction notably decreases with the increase in the Hamming distance, reinforcing that the effectiveness of downstream predictions, like $\Delta\DeltaG$ of mutation effect, is heavily reliant on the composition and filtering of the training data.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/influence.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <p>Consequently, accurate predictions of PLMs are significantly influenced by preprocessing and selection of the training data.</p> <h3 id="a-novel-robust-fine-tuning-strategy">A novel, robust fine-tuning strategy</h3> <p>Intuitively, training PLMs on a diverse, broadly sampled dataset should enhance model generalization. However, this expectation in general contrasts with empirical observations, where typically only a small subset of training sequences strongly influences the resultant sequence likelihood predictions. A majority of sequences exert minimal or negligible impact, thus introducing substantial bias if not addressed explicitly.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/power_tail.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <p>Therefore, relying solely on a naïve fine-tuning procedure may fail to correct underlying biases, especially for downstream tasks involving underrepresented sequences. To counteract these biases, the authors propose a straightforward yet robust fine-tuning pipeline:</p> <ol> <li>Evaluate PLL scores using a pretrained PLM.</li> <li>If the PLL score is significantly low, filter out homologs from the fine-tuning dataset, and then implement Evo-tuning.</li> <li>Otherwise, fine-tune your-selected PLM directly on the available data. Applying this method to the ESM2 model reveals tangible performance improvements for underrepresented proteins as fine-tuning epochs increase. However, there exists a critical threshold, $\epsilon$, beyond which additional fine-tuning yields diminishing returns or even harms model performance.</li> </ol> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/naive_tuning.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <p>Empirical results indicate an optimal fine-tuning threshold at $\epsilon = -1.4$ that it confers ESM-2 650M model the maximum performance improvement compared to its baseline and naïvely evo-tuned counterparts. Importantly, this fine-tuning strategy generalizes effectively across diverse downstream tasks beyond zero-shot mutation-effect prediction.</p> <figure> <img class="invert" style="max-width: 100%; height: auto;" src="/assets/img/2025-04-28-20243287/finetuning.png" frameborder="0" scrolling="no"> <figcaption class="caption"> Pseudo Likelihoods vs. Single-Inference Pseudo Likelihoods across different kingdoms. </figcaption> </figure> <h2 id="discussions">Discussions</h2> <p>GraphGPS, Neural Atom</p> <h2 id="conclusion">Conclusion</h2> <p>Future work I have not discussed in this post, but several other papers also tried to tackle the same long range problem, one with naively relating attention with Ewald principle (Neural Atom, ICLR2024) and …</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-04-28-20243287.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongje Moon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>