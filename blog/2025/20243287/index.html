<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI810 Blog Post (20243287) | Dongje Moon </title> <meta name="author" content="Dongje Moon"> <meta name="description" content="This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dongjemoon.github.io/blog/2025/20243287/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "AI810 Blog Post (20243287)",
            "description": "This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025).",
            "published": "April 28, 2025",
            "authors": [
              
              {
                "author": "Dongje Moon",
                "authorURL": "https://dongjemoon.github.io/",
                "affiliations": [
                  {
                    "name": "KAIST",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dongje</span> Moon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>AI810 Blog Post (20243287)</h1> <p>This post reviews two recent papers: Ewald-based Long-Range Message Passing for Molecular Graphs (ICML 2023) and Protein Language Model Fitness is a Matter of Preference (ICLR 2025).</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#motivation-for-this-review">Motivation for this review</a> </div> <div> <a href="#ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</a> </div> <ul> <li> <a href="#why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</a> </li> <li> <a href="#what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</a> </li> <li> <a href="#ewald-message-passing">Ewald Message Passing</a> </li> <li> <a href="#results-on-long-ranged-interaction">Results on long-ranged interaction</a> </li> <li> <a href="#limitation">Limitation</a> </li> </ul> <div> <a href="#protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</a> </div> <ul> <li> <a href="#recent-works-reveal-limitations-of-sota-plms">Recent works reveal limitations of SoTA PLMs</a> </li> <li> <a href="#what-is-preference-why-preference">What is preference? Why preference?</a> </li> <li> <a href="#single-inference-pseudo-log-likelihood">Single-inference pseudo log likelihood</a> </li> <li> <a href="#influence-diminishes-with-edit-distance">Influence diminishes with edit distance</a> </li> <li> <a href="#a-novel-fine-tuning-pipeline-based-on-pll">A novel fine-tuning pipeline based on PLL</a> </li> </ul> <div> <a href="#conclusions-and-discussions">Conclusions and discussions</a> </div> </nav> </d-contents> <h2 id="motivation-for-this-review">Motivation for this review</h2> <p>Turning a linear string of amino acids into a representation that truly reflects protein behavior remains a frontier in computational biology. Early encodings—one-hot vectors, BLOSUM62 matrices, and AAIndex descriptors—laid the groundwork, but none fully capture the subtle, long-range dependencies that govern folding and function. In response, protein language models (PLMs) inspired by breakthroughs in natural-language processing have scaled sequence representation to an unprecedented level, yet adapting those architectures to the chemistry of amino acids—and extending them to atom-level resolution—continues to pose a significant challenge. Meanwhile, graph neural networks (GNN) convert molecular structures into graphs and rely on message-passing schemes that typically truncate atom interactions after only a few bond-length hops, even though electrostatic and dispersion forces span entire macromolecules. Similarly, how to effectively leverage multi-scale PLMs into a particular protein domain remains elusive.</p> <p>With this blog post, we examine two recent studies designed to close these gaps. First, we revisit Ewald Summation—long established in molecular dynamics for treating infinite periodic systems—and show how it can be reinterpreted as a form of long-range message passing, enabling graph neural networks to capture far-flung interactions without excessive computational costs. Then, we explore a fine-tuning paradigm for PLMs that how we can strategically adapt large, pretrained PLMs into specific protein domain without compromising their performance. Together, these works highlight an overarching theme: the importance of bridging theoretical rigor with practical scalability. Whether it’s importing principled physics into graph neural networks or constructing a statistics-based pipeline for fine-tuning pretrained PLMs, both efforts illustrate that grounding AI tools in the right assumptions is essential—and challenging those assumptions often leads to the deepest insights.</p> <h2 id="ewald-based-long-range-message-passing-for-molecular-graphs">Ewald-based Long-Range Message Passing for Molecular Graphs</h2> <h3 id="why-does-long-range-matters-in-gnn">Why does long-range matters in GNN?</h3> <p>In standard Message Passing Neural Networks (MPNNs) for molecular systems, each atom’s representation is updated by aggregating information from the neighbors within a <strong>fixed distance cutoff</strong>. While this effectively captures local chemical interactions especially at the Dalton scale, it systematically neglects long-range forces such as electrostatics, dispersion, and allosteric effects in larger molecules (often kilodalton to megadalton scale).</p> <p>This locality bias in large graphs gives to over-squashing problem of GNNs <d-cite key="alon2021bottleneck"></d-cite>. Information from distant nodes must traverse many intermediate aggregations, compressing rich signals into narrow bottlenecks and degrading the network’s ability to learn non-local dependencies. Therefore, multiple strategies, including sensitivity analysis and graph-rewiring, have been addressed to solve this problem in long-range interaction between a source and target vertex.</p> <p>In particular, we will explore how the <strong>Ewald Summation</strong> principle can be reformulated as an agnostic message-passing module to directly mitigate over-squashing and endow geometric GNNs with a true global context.</p> <h3 id="what-is-ewald-summation-and-where-is-it-useful">What is Ewald Summation and where is it useful?</h3> <p>Before we define Ewald Summation, we must first introduce the notion of <strong>Periodic Boundary Conditions</strong> (PBC) and <strong>Supercell lattice</strong>. Under PBCs, a finite simulation box is conceptually tiled infinitely in every direction (see Figure 1). This wrapping ensures every atom experience a uniform environment and eliminates any edge artifacts. The Supercell lattice specifies the integer combinations of the box’s basis vectors that generate all image cells (the replicas)—atom embeddings in the primary cell thus can be replicated implicitly across these translations when computing interactions.</p> <p>Without PBCs, a naïve sum of Coulomb potential of ions with distance cutoff over an infinite lattice is only conditionally convergent and thus computationally infeasible. (e.g. the long tail problem from the inversely proportional to the distance). The solution lies in recognizing that the direct lattice can be approximated using reciprocal lattice in Fourier space with a frequency cutoff. Ewald Summation exploits this dual structure by decomposing the problematic infinite sum ($\Phi(r)$) into two rapidly, absolutely convergent series: short-range interaction ($\Phi^{sr}(r)$) and long-range interaction ($\Phi^{lr}(r)$), which can be expressed in the Fourier space as $\hat\Phi^{lr}(||\dot||)$.</p> <p>\begin{equation} \begin{split} \Phi(r) = \Phi_{\mathrm{sr}}(r) + \Phi_{\mathrm{lr}}(r), \ <br> r = \lVert \mathbf{x}_i - \mathbf{x}_j\rVert. \end{split} \end{equation}</p> <h3 id="ewald-message-passing">Ewald Message Passing</h3> <p>Authors of this paper adopt this exact decomposition of $\Phi(r)$ as the foundation of the Ewald Message Passing (EMP) module, which can serve as an augmentation on top of existing MPNN architectures. The short-range convolution, $\Phi^{sr}(r)$, makes use of the continuous-filter convolution employed by any geometric GNN that implements distance-dependent message function (e.g. SchNet, PaiNN, DimeNet++, GemNet-T). Here, an atom gathers information from all neighbors within a prescribed Euclidean cutoff by applying a learnable radial filter to inter-atomic distances. This preserves the model’s capacity to capture local chemical phenomena without any explicit modification into the pre-existing GNN architecture.</p> <p>To implement the long-range stream in a graph context, they first define a <strong>structure factor</strong>, $s_{k}$, a notion from crystallography, as a Fourier space representation of atom embeddings, which defines a measure of how node embeddings, $h_{j}$, are collectively scatter at frequency <em>k</em> (i.e. a weighted sum over <em>all</em> supercell embeddings). To obtain $s_{k}$, $h_{j}$’s are projected to the reciprocal space by computing over a fixed, learnable set of wavevectors {<em>k</em>}</p> <p>\begin{equation} \label{eq:1} \begin{split} s_{\mathbf{k}} &amp;= \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}^{T}\mathbf{x}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Once in the reciprocal space, $\Lambda^{\prime}$, each mode is modulated by a learnable frequency filter, $\hat{\Phi}_{\mathrm{lr}}(||\mathbf{k}||)$, yielding</p> <p>\begin{equation} \begin{split} \hat{M}^{\mathrm{lr}}(\mathbf{x_i}) &amp;= s_{\mathbf{k\in\Lambda^{\prime}}}\;\hat{\Phi}_{\mathrm{lr}}\bigl(||\mathbf{k}||\bigr)\,. \end{split} \end{equation}</p> <p>Next, $s_{k}$ is projected back into the real position space through inverse transformation</p> <p>\begin{equation} \begin{split} M_i^{\mathrm{lr}} &amp;= \sum_{\mathbf{k}} \hat{M_k}^{\mathrm{lr}}\;exp({\,i\,\mathbf{k}\cdot \mathbf{x}_i})\,, \end{split} \end{equation}</p> <p>Finally, $M^{lr}$ is added to $M^{sr}$, forming a complete update cycle for a single layer of GNN-EMP architecture</p> <p>\begin{equation} \begin{split} h_i^{(\ell+1)} &amp;= \frac{1}{\sqrt{3}}\Bigl(h_i^{(\ell)} + f_{\mathrm{upd}}^{\mathrm{sr}}\bigl(M_i^{\mathrm{sr}}\bigr) + f_{\mathrm{upd}}^{\mathrm{lr}}\bigl(M_i^{\mathrm{lr}}\bigr)\Bigr)\,. \end{split} \end{equation} Authors employ two mode-generation schemes depending on whether the system is periodic or aperiodic. For an aperiodic case, there is no reciprocal lattice $\Lambda^{\prime}$, so the structure factor, $s_k$ map is first regarded as a continuous function from $\mathbb{R}^3$ to the D-dimensional complex numbers.</p> <p>\begin{equation} \begin{split} s(k): \mathbb{R}^3 &amp;\to \mathbb{C}^d,<br> \mathbf{k} &amp;\mapsto \sum_{j} h_{j}\,\exp\bigl(-i\,\mathbf{k}!\cdot!\mathbf{r}_{j}\bigr)\,. \end{split} \end{equation}</p> <p>Because the real-space supercell size and the spacing of admissible k-vectors are inversely related, this field must be sampled on a finite grid. Therefore, they place a uniform voxel grid of <em>k</em> wave-vectors and replace all continuous functions by its voxel averages. To prevent aliasing and to guarantee rotational invariance, the grid is defined in a co-rotating frame obtained from principal component analysis (PCA) of the atomic point cloud: the three PCA eigen-vectors (ordered by descending eigen-value) form an orthonormal basis that rotates with the molecule. Rotating the molecule in real space then rotates the voxel grid in the reciprocal space, keeping the sampled modes and their learned filter unaffected by the global orientation.</p> <p>Similarly, <strong>frequency filters</strong>, $\hat\Phi(||\dot||)$, are parameterized differently in the two settings. In aperiodic cases, they employ the same radial-basis parametrization used in ScheNet: $\hat\Phi(||\k||)$ is expressed as a linear combination of Gaussian radial basis functions whose centers lie within k-cutoff radius, $c_k$ in Fourier space. Because the filter depends on the magnitude of $||\k||$, it is rotation-invariant once the molecule has been aligned to its PCA frame. However, for periodic systems, a radial envelope would be overly restrictive; the periodic boundary conditions carry valuable geometric information. Instead, the filter is non-radial: a separate complex coefficient is learned for every point-symmetry orbit in the reciprocal lattice $\Lambda^{\prime}$. Because the reciprocal lattice co-rotates with the supercell lattice, the learned weights remain rotation-invariant, even though the filter is no longer radial.</p> <h3 id="results-on-long-range-interaction">Results on long-range interaction</h3> <p>The authors evaluate EMP module on two public data sets with contrasting geometry: OC20 — 133M surface-adsorbate structures from density functional theory (DFT) relaxations, ~50-200 atoms each OE62 — 62k organic molecules, energies include D3 dispersion, ~50-100 atoms each As baselines, four geometric GNN models are considered: the node-based models SchNet and PaiNN, and the edge-based models DimeNet++ and GemNet-T. Each EMP variant is parameter-matched to its baseline by reducing the hidden size so that both networks contain the same total number of trainable parameters. Energy mean absolute error (EMAE) is used as the evaluation metric. Adding EMP consistently lowers EMAE on all four baseline models. With real space cut-offs between 6 and 12 angstroms and the corresponding Fourier cut-offs, For all models, EMP with cutoffs between 6 and 12 angstroms improves the performance compare to the base line models with additional parameters so that same number of parameters are used in the comparing conditions, demonstrating that improvements from using EMP is not just merely due to extra parameters (see Figure 2). Furthermore, EMP sacrifices minimal runtime, which is just in a scale of milliseconds. However, for OE62 dataset edge models’ improvements are trivial because relatively small cutoffs is used because 12 angstrom cutoff already saturates most of the graphs in the dataset to have full connectivity. In order to validate whether EMP actually learns a long-range interaction, authors compare EMP with a hand-crafted long-range correction specifically with correction method using density functional theory (DFT-D3). Corrective impact of EMP largely correlates with the amplitude of D3 correction (see Figure 3). And, it surpasses the D3 correction base line at some point as shown in SchNet and DimeNet++, meaning that EMP not only learns about Van Dal Waals force without any hand-crafting but also other things that contribute to better performance.</p> <p>Limitation Compare to energy, forces are more dominated by the high wavelength terms rather than low wavelength terms. This is what not addressed by the EMP, because EMP focuses on the low wavelength terms within the prescribed frequency cutoff. Therefore, EMP is more of an energy correction scheme.</p> <h2 id="protein-language-model-fitness-is-a-matter-of-preference">Protein Language Model Fitness is a Matter of Preference</h2> <h2 id="discussions">Discussions</h2> <h2 id="conclusion">Conclusion</h2> <p>Future work I have not discussed in this post, but several other papers also tried to tackle the same long range problem, one with naively relating attention with Ewald principle (Neural Atom, ICLR2024) and …</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-04-28-20243287.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongje Moon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>